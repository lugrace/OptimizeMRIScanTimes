These are the files that I changed the paths from /mnt/raid5/david to /mnt/data/grace

dopamine/recon_env/reinforce.py:32:logdir = os.path.join('/mnt/raid5/davidyzeng/policy',currstr)
dopamine/recon_env/generate_image.py:16:data_file = '/mnt/raid5/davidyzeng/unroll_data_raw/pytorch_data.h5'
run_dqn.sh:4:python -um dopamine.atari.train --agent_name=dqn --base_dir=/mnt/raid5/davidyzeng/dopamine/recon$date_str --gin_files='dopamine/agents/dqn/configs/dqn.gin'
val_dqn.sh:6:python -um dopamine.atari.train --agent_name=dqn --base_dir=/mnt/raid5/davidyzeng/dopamine/recon$date_str --gin_files='dopamine/agents/dqn/configs/dqn.gin'
val_rainbow.sh:6:python -um dopamine.atari.train --agent_name=rainbow --base_dir=/mnt/raid5/davidyzeng/dopamine/rainbow$date_str --gin_files='dopamine/agents/rainbow/configs/rainbow.gin'

and then also run_rainbow.sh

also changed python to python3 in run_rainbow.sh

also commented out bartwrap in recon_env.py

also added the second sys path append in recon_env.py

originally (in /dopamine/dopamine/agents/rainbow/configs):
learning_rate = 0.0000625
epsilon = 00015
iterations = 200
training steps = 250000
evaluation steps = 125000
max steps per episode = 27000
batch_size = 32
replay_capacity = 100000

the logdir is either /mnt/data/grace/policy or /mnt/raid5/davidyzeng/policy

we are not using the GPU, so I see in dopamine/agents/rainbow/rainbow_agent we are using
tf_device='/cpu:*' but we should probably use '/gpu:0' because we only use
/cpu for non-GPU version
everywhere else, we see that we use the gpu (except for the dqn agent)
oh wait let's also change /dopamine/replay_memory/circular_replay_buffer.py

changed export CUDA-VISIBLE_DEVICES = 0 from 1 in run_rainbow.sh

well apparently we're using the priotized relay scheme
and we r getting segfaults cuz lack of memory so lets 
reduce the replay_memory from 1,000,000 to 50,000

also changed tf.func to tf.function in prioritized

^^ changed the tf.func and replay_memory back cuz it was breaking once i only
changed the batch_size to 64

